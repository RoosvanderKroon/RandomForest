########### practice 1 with QGIS polygons and rf classification in R ##############

# PART 1 - QGIS #

# First get raster image in QGIS --> my case I used google satellite and the Raster tool to crop area of interest into raster image and 
# save it as "Training_data.tif" file
# then start making polygons of different landcover site, not too big! with attribute "class"
# make sure that the shapefile and rasterfile use the same CRS!
# add classes in layer properties, symbology so see different classes (agriculture, forest, field)
# export layer and save as shapefile
# let's go to R

# PART 2 - R #

library (raster)
library (ggplot2)
library (caret)
library (randomForest)
# set working directory setwd("C://your_path/.../...")

# assign rasterfile with brick function (multiple layers)
aoi <- brick("Training_data.tif")
# assign shapefile data and check if raster file and shapefile have the same CRS
datapolygons <- shapefile("landclassification2.shp")

# start plotting, first the rasterfile
plotRGB(aoi, r = 1, g = 2, b = 3, stretch = "lin")
plot(datapolygons, col= "red", add= TRUE)

# rename spectral bands 
names(aoi)<- c("red","green", "blue")
names(aoi)

# transform into factor and see classifications
levels(as.factor(datapolygons$class))
#extract data and turn into dataframe
rasterdata <- extract(aoi, datapolygons, df = TRUE) #make sure that tidyr package is turned off as it also has extract function and leads to error

# create extra column landcover class corresponding to ID, i.e. the polygons classifications
rasterdata$landcover <- as.factor( datapolygons$class[match(rasterdata$ID, seq(nrow(datapolygons)))])
# Remove the ID column, not needed anymore
rasterdata<- rasterdata[-1]

## see how many cases for each landcover type
summary(rasterdata$landcover)
# drawing bootstrap sample based on the landcover type with the least cases
smp.size <- rep(min(summary(rasterdata$cl)), nlevels(rasterdata$cl))
rfmodel <- tuneRF(x = rasterdata[-ncol(rasterdata)],
                   y = rasterdata$cl,
                   sampsize = smp.size,
                   strata = rasterdata$cl,
                   ntree = 250,
                   importance = TRUE,
                   doBest = TRUE)
# output: 
#mtry = 1  OOB error = 8.84% 
#Searching left ...
#Searching right ...
#mtry = 2 	OOB error = 8.9% 
#-0.006841369 0.05 
Error: cannot allocate vector of size 2.0 Gb

########## PART 2.1 subsetting data too decrease size
## too big 
## Error messages beginning '⁠cannot allocate vector of size⁠' indicate a failure to obtain memory, either because the 
# size exceeded the address-space limit for a process or, more likely, because the system was unable to provide the memory

########
# Try the gc() command, used for garbage collection and increase memory limit
gc()
memory.limit() ## to show current memory limit, 3458
memory.limit(9999999999) ## to increase memory size
# run tuneRF again, but still vector too big to allocate this time even 4.0 GB (another time 8110 GB)
23-11-2023
#### Lots of googling, simply do not have enough RAM
# readyboost with flashdrives not possible on laptop as the system is already "fast enough"

# subsetting the data ###### 
subset<- rasterdata[unlist(tapply(1:nrow(rasterdata),rasterdata$landcover,function(x) sample(x,100000))),]

# so for the three landcover classes random take 100000 rows, so total subset df is 300000, roughly third of orignal df

# try rf with splitting data in train and test
subset$landcover <- as.factor(subset$landcover)

set.seed(123)
ind<- sample(2, nrow(subset), replace=TRUE, prob = c(0.7,0.3))
train<- subset[ind==1,]
test<- subset[ind==2,]
rfm <- randomForest(landcover~., data=train)
# Error: cannot allocate vector of size 1.6 Gb

# Let's try first one with tuneRF
smp.size <- rep(min(summary(subset$landcover)), nlevels(subset$landcover))
rfmodel <- tuneRF(x = subset[-ncol(subset)],
                   y = subset$landcover,
                   sampsize = smp.size,
                   strata = subset$landcover,
                   ntree = 250,
                   importance = TRUE,
                   doBest = TRUE)
#  Error: cannot allocate vector of size 2.2 Gb doBest = TRUE)
# 60.000 is biggest, so 180.000 in total, without error 
subset<- rasterdata[unlist(tapply(1:nrow(rasterdata),rasterdata$landcover,function(x) sample(x,60000))),]
subset$landcover <- as.factor(subset$landcover)

set.seed(123)
ind<- sample(2, nrow(subset), replace=TRUE, prob = c(0.7,0.3))
train<- subset[ind==1,]
test<- subset[ind==2,]
rfm <- randomForest(landcover~., data=train)
print(rfm)
# ntrees 500, n variable at each split 1, OOB 10.28% meaning that around 90% of model is accurate

p1<- predict(rfm, train)
confusionMatrix(p1,train$landcover)
# Accuracy : 0.9297          
   #              95% CI : (0.9283, 0.9311)


# let's do it on test data to see for real
p2<- predict(rfm, test)
confusionMatrix(p2, test$landcover

#Accuracy : 0.8992          
 #                95% CI : (0.8966, 0.9017)

####### PART 3 -Running the model ###########33
result <- predict(aoi,rfm,filename = "classification.tif",overwrite = TRUE)
## takes about 1,5 hour

 plot(result, axes = FALSE, box = FALSE, col = c("#964b00", # agri "#013220", # forest,"#90ee90" # field))
                
# plot is small, stays small when adjusting the panes, so need to resize





