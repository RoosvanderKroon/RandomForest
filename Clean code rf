# saved polygons and raster image of area of interest saved from QGIS

library (raster)
library (ggplot2)
library (caret)
library (randomForest)
# set working directory 

# assign rasterfile with brick function (multiple layers)
aoi <- brick("aoi.tif")
# assign shapefile data and check if raster file and shapefile have the same CRS
datapolygons <- shapefile("polygonslandcover.shp")

# plot the map with the shapefile of polygons
plotRGB(aoi, r = 1, g = 2, b = 3, stretch = "lin")
plot(datapolygons, col= "red", add= TRUE)

# rename spectral bands 
names(aoi)<- c("red","green", "blue")
names(aoi)

# transform into factor and see classifications
levels(as.factor(datapolygons$class))
#extract data and turn into dataframe
rasterdata <- extract(aoi, datapolygons, df = TRUE) 

# create extra column landcover class corresponding to column "ID", i.e. the polygons classifications
rasterdata$landcover <- as.factor( datapolygons$class[match(rasterdata$ID, seq(nrow(datapolygons)))])
# Remove the ID column, not needed anymore
rasterdata<- rasterdata[-1]

# see how many cases for each landcover type
summary(rasterdata$landcover)
# drawing bootstrap sample based on the landcover type with the least cases
smp.size <- rep(min(summary(rasterdata$cl)), nlevels(rasterdata$cl))
rfmodel <- tuneRF(x = rasterdata[-ncol(rasterdata)],
                   y = rasterdata$cl,
                   sampsize = smp.size,
                   strata = rasterdata$cl,
                   ntree = 250,
                   importance = TRUE,
                   doBest = TRUE)
# output: 
#mtry = 1  OOB error = 8.84% 
#Searching left ...
#Searching right ...
#mtry = 2 	OOB error = 8.9% 
#-0.006841369 0.05 
Error: cannot allocate vector of size 2.0 Gb

########## PART 2.1 subsetting data too decrease size
## too big 
## Error messages beginning '⁠cannot allocate vector of size⁠' indicate a failure to obtain memory, either because the 
# size exceeded the address-space limit for a process or, more likely, because the system was unable to provide the memory

########
# Try the gc() command, used for garbage collection and increase memory limit
gc()
memory.limit() ## to show current memory limit, 3458
memory.limit(9999999999) ## to increase memory size

# subsetting the data ###### 
subset<- rasterdata[unlist(tapply(1:nrow(rasterdata),rasterdata$landcover,function(x) sample(x,15000))),]

# try rf with splitting data in train and test
subset$landcover <- as.factor(subset$landcover)

set.seed(123)
ind<- sample(2, nrow(subset), replace=TRUE, prob = c(0.7,0.3))
train<- subset[ind==1,]
test<- subset[ind==2,]
rfm <- randomForest(landcover~., data=train)


print(rfm)
# ntrees 500, n variable at each split 1, OOB 10.28% meaning that around 90% of model is accurate


# let's do it on test data to see for real
p1<- predict(rfm, test)
confusionMatrix(p1, test$landcover)

#Accuracy : 0.8992          
 #                95% CI : (0.8966, 0.9017)

####### PART 3 -Running the model and plotting result ###########33
result <- predict(aoi,rfm,filename = "classification.tif",overwrite = TRUE)
## takes about 1,5 hour

 plot(result, axes = FALSE, box = FALSE, col = c("brown, # agri "light green", # field,"dark green" # forest))
                
